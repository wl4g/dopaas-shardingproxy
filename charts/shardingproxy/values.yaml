## This is the spec definition of ShardingProxy instance deployed in Kubernetes.
shardingproxy:
  # ========================================== Begin ShardingProxy Deployment ================================================
  image:
    repository: wl4g/shardingproxy
    # tag: latest
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ##
    pullSecrets:
    # - myRegistryKeySecretName

  ## It is recommended to have odd number of nodes in a cluster, otherwise the shardingproxy cluster cannot be automatically healed in case of net-split.
  autoscaling:
    enabled: true
    replicaCount: 2

  # The update strategy for deployments with persistent volumes(jobservice, registry
  # and chartmuseum): "RollingUpdate" or "Recreate"
  # Set it as "Recreate" when "RWM" for volumes isn't supported
  updateStrategy:
    type: RollingUpdate

  # The name of a secret in the same kubernetes namespace which contains values to
  # be added to the environment (must be manually created)
  # This can be useful for passwords and logins, etc.

  # envFromSecret: "shardingproxy-secrets"

  ## Additional deployment annotations
  podAnnotations: 
    prometheus.io/scrape: "true"
    prometheus.io/port: "10108"

  persistence:
    enabled: false
    size: 20Mi
    ## If defined, volume.beta.kubernetes.io/storage-class: <storageClass>
    ## Default: volume.alpha.kubernetes.io/storage-class: default
    # storageClass: "-"
    accessMode: ReadWriteOnce
    ## Existing PersistentVolumeClaims
    ## The value is evaluated as a template
    ## So, for example, the name can depend on .Release or .Chart
    existingClaim: ""

  resources:
    enabled: false
    limits:
      cpu: 500m
      memory: 1024Mi
    requests:
      cpu: 500m
      memory: 1024Mi

  # Containers that run before the creation of ShardingProxy containers. They can contain utilities or setup scripts.
  initContainers: {}
    # - name: mysql-probe
    #   image: alpine
    #   command: ["sh", "-c", "for i in $(seq 1 300); do nc -zvw1 mysql 3306 && exit 0 || sleep 3; done; exit 1"]

  podSecurityContext:
    enabled: true
    fsGroup: 1000
    fsGroupChangePolicy: Always
    runAsUser: 1000
    supplementalGroups:
      - 1000

  containerSecurityContext:
    enabled: false
    runAsNonRoot: true
    runAsUser: 1000

  nodeSelector: {}

  tolerations: []

  affinity: {}

  # ========================================== End ShardingProxy Deployment ================================================

  # ========================================== Begin ShardingProxy ConfigMap ===============================================
  ## ShardingProxy agent configuration, see: https://github.com/wl4g/xcloud-shardingproxy/blob/master/xcloud-shardingproxy-starter/src/main/resources/agent/conf/agent.yaml
  agentConfig: |-
    applicationName: cn_south1_a1_shardingproxy_0
    ignoredPluginNames:
      - Jaeger
      - Zipkin
      - Opentracing
    plugins:
      Logging:
        props:
          LEVEL: "DEBUG"
      Prometheus:
        host:  "0.0.0.0"
        port: 10108
        props:
          JVM_INFORMATION_COLLECTOR_ENABLED : "true"
      OpenTelemetry:
        props:
          otel.resource.attributes: "service.name=cn-south1-a1-shardingproxy,service.namespace=shardingproxy"
          otel.traces.exporter: "jaeger"
          otel.traces.sampler: "parentbased_always_on"
          otel.traces.sampler.arg: "0.99"
          otel.span.attribute.count.limit: "128"
          otel.span.event.count.limit: "128"
          otel.span.link.count.limit: "128"
          otel.bsp.schedule.delay: "5000"
          otel.bsp.max.queue.size: "2048"
          otel.bsp.max.export.batch.size: "512"
          otel.bsp.export.timeout: "30000"
          otel.exporter.jaeger.endpoint: "http://jaeger.shardingproxy.svc.cluster.local:14250"
          otel.exporter.jaeger.timeout: "10000"

  ## ShardingProxy sharding configuration, example see: https://github.com/wl4g/xcloud-shardingproxy/blob/master/xcloud-shardingproxy-starter/src/main/resources/example/
  shardingConfigs:
    readwrite-warehousedb: |-
      schemaName: warehousedb
      extensionDefaultDataSource:
        username: root
        password: 123456
        connectionTimeoutMilliseconds: 30000
        idleTimeoutMilliseconds: 60000
        maxLifetimeMilliseconds: 1800000
        maxPoolSize: 50
        minPoolSize: 1
      dataSources:
        ds_warehousedb_r0z0mgr0i0:
          url: 'jdbc:mysql://n0.rds.local:3306/warehousedb?serverTimezone=UTC&useSSL=false&allowMultiQueries=true&characterEncoding=utf-8'
        ds_warehousedb_r0z0mgr0i1:
          url: 'jdbc:mysql://n1.rds.local:3306/warehousedb?serverTimezone=UTC&useSSL=false&allowMultiQueries=true&characterEncoding=utf-8'
        ds_warehousedb_r0z0mgr0i2:
          url: 'jdbc:mysql://n2.rds.local:3306/warehousedb?serverTimezone=UTC&useSSL=false&allowMultiQueries=true&characterEncoding=utf-8'
      rules:
      - !READWRITE_SPLITTING
        dataSources:
          rw_warehousedb_r0z0mgr0:
            writeDataSourceName: ds_warehousedb_r0z0mgr0i0
            readDataSourceNames:
              - ds_warehousedb_r0z0mgr0i1
              - ds_warehousedb_r0z0mgr0i2
            loadBalancerName: r_lb_0
        loadBalancers:
          r_lb_0:
            type: RANDOM
      - !DB_DISCOVERY
         dataSources:
           ha_warehousedb_r0z0mgr0:
             discoveryTypeName: default_dis_mgr
             discoveryHeartbeatName: default_mgr_heartbeat
             dataSourceNames:
               - ds_warehousedb_r0z0mgr0i0
               - ds_warehousedb_r0z0mgr0i1
               - ds_warehousedb_r0z0mgr0i2
         discoveryHeartbeats:
           default_mgr_heartbeat:
             props:
               keep-alive-cron: '0/5 * * * * ?'
         discoveryTypes:
           default_dis_mgr:
             type: MGR
             props:
               group-name: 5db40c3c-180c-11e9-afbf-005056ac6820
               extensionDiscoveryConfigJson: |-
                 {
                   "memberHostMappings": [{
                       "rds-mgr-0:3306": [
                           "n0.rds.local:3306"
                       ]
                   }, {
                       "rds-mgr-1:3306": [
                           "n1.rds.local:3306"
                       ]
                   }, {
                       "rds-mgr-2:3306": [
                           "n2.rds.local:3306"
                       ]
                   }]
                 }
  # ========================================== End ShardingProxy ConfigMap ===============================================

  # ========================================== Begin ShardingProxy Service ===============================================
  service:
    ## Service type
    ##
    type: ClusterIP
    ## Port for Proxy JDBC
    ##
    proxy: 3308
    ## Port for dashboard
    ##
    dashboard: 18083
    ## Port for prometheus API
    ##
    prometheus: 8081
    ## Specify the nodePort(s) value for the LoadBalancer and NodePort service types.
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
    ##
    nodePorts:
      proxy:
      dashboard:
      prometheus:
    ## Set the LoadBalancer service type to internal only.
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
    ##
    # loadBalancerIP:
    ## Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## Example:
    ## loadBalancerSourceRanges:
    ## - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## Set the ExternalIPs
    ##
    externalIPs: []
    ## Provide any additional annotations which may be required. Evaluated as a template
    ##
    annotations: {}

  ingress:
    ## ingress for ShardingProxy Dashboard
    dashboard:
      enabled: false
      ingressClassName: nginx
      annotations: {}
        # kubernetes.io/ingress.class: nginx
        # kubernetes.io/tls-acme: "true"
      path: /
      hosts:
      - dashboard.shardingproxy.svc.cluster.local
      tls: []

    ## ingress for ShardingProxy prometheus API
    prometheus:
      enabled: false
      ingressClassName: nginx
      annotations: {}
        # kubernetes.io/ingress.class: nginx
        # kubernetes.io/tls-acme: "true"
      path: /
      hosts:
      - prometheus.shardingproxy.svc.cluster.local
      tls: []

  # ========================================== End ShardingProxy Service ===============================================
